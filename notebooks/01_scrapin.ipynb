{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909a9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df25cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Generar URL\n",
    "drivers_df = pd.read_csv(\"../data/raw/drivers.csv\")\n",
    "\n",
    "drivers_df[\"wikipedia_url\"] = drivers_df[\"url\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Lewis Hamilton... \n",
      "Scraping Nick Heidfeld... \n",
      "Scraping Nico Rosberg... \n",
      "Scraping Fernando Alonso... \n",
      "Scraping Heikki Kovalainen... \n",
      "Scraping Kazuki Nakajima... \n",
      "Scraping Sébastien Bourdais... \n",
      "Scraping Kimi Räikkönen... \n",
      "Scraping Robert Kubica... \n",
      "Scraping Timo Glock... \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#2 Scrapear cada piloto\n",
    "scraped_data = []\n",
    "\n",
    "for index, row in drivers_df.head(10).iterrows():    #Voy a probar primer con 5 pilotos, luego lo escalo\n",
    "     driverId = row[\"driverId\"]\n",
    "     url = row[\"wikipedia_url\"]\n",
    "\n",
    "     print(f\"Scraping {row[\"forename\"]} {row[\"surname\"]}... \")\n",
    "     \n",
    "\n",
    "     try:\n",
    "          headers = {\"User-Agent\": \"UniversityProject/1.0 (Educational Purpose)\"}\n",
    "          response = requests.get(url, headers = headers)\n",
    "\n",
    "          if response.status_code == 200:\n",
    "               soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "               infobox = soup.find(\"table\", {\"class\": \"infobox\"})\n",
    "\n",
    "               if infobox:\n",
    "                    pilot_data = {\"driverId\": driverId}\n",
    "                    rows = infobox.find_all(\"tr\")\n",
    "\n",
    "                    for row_html in rows:\n",
    "                         header = row_html.find(\"th\")\n",
    "                         value = row_html.find(\"td\")\n",
    "\n",
    "                         if header and value:\n",
    "                              key = header.get_text(strip=True)\n",
    "                              val = value.get_text(strip=True)\n",
    "                              pilot_data[key] = val\n",
    "                    \n",
    "                    scraped_data.append(pilot_data)\n",
    "\n",
    "          time.sleep(2)\n",
    "\n",
    "     except Exception as e:\n",
    "          print(f\"Error con {url}: {e}\")\n",
    "          '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d1b844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Lewis Hamilton...\n",
      "Scraping Nick Heidfeld...\n",
      "Scraping Nico Rosberg...\n",
      "Scraping Fernando Alonso...\n",
      "Scraping Heikki Kovalainen...\n",
      "Scraping Kazuki Nakajima...\n",
      "Scraping Sébastien Bourdais...\n",
      "Scraping Kimi Räikkönen...\n",
      "Scraping Robert Kubica...\n",
      "Scraping Timo Glock...\n"
     ]
    }
   ],
   "source": [
    "scraped_data = []\n",
    "\n",
    "campos_buscados = {\n",
    "        \"Championships\",\n",
    "        \"Wins\",\n",
    "        \"Podiums\",\n",
    "        \"Carreer points\",\n",
    "        \"Pole positions\",\n",
    "        \"Fastest laps\",\n",
    "        \"Entries\",\n",
    "        \"First entry\",\n",
    "        \"First win\",\n",
    "        \"Last entry\",\n",
    "        \"Last win\"\n",
    "}\n",
    "\n",
    "for index, row in drivers_df.head(10).iterrows():\n",
    "    driverId = row[\"driverId\"]\n",
    "    url = row[\"wikipedia_url\"]\n",
    "\n",
    "    print(f\"Scraping {row[\"forename\"]} {row[\"surname\"]}...\")\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"UniversityProject/1.0 (Educational Purpose)\"}\n",
    "\n",
    "        response = requests.get(url, headers = headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            infobox = soup.find(\"table\", {\"class\": \"infobox\"})\n",
    "\n",
    "            if infobox:\n",
    "                pilot_data = {\"driverId\": driverId}\n",
    "                rows = infobox.find_all(\"th\")\n",
    "\n",
    "                for row_html in rows: \n",
    "                    header = row_html.find(\"th\")\n",
    "                    value = row_html.find(\"td\")\n",
    "\n",
    "                    if header and value:\n",
    "                        key = header.get_text(strip=True)\n",
    "\n",
    "                        if key in campos_buscados:\n",
    "                            val = value.get_text(strip=True)\n",
    "                            pilot_data[key] = val\n",
    "\n",
    "                scraped_data.append(pilot_data)\n",
    "            else:\n",
    "                print(f\"No se encontró infobox para {driverId}\")\n",
    "\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con {url}: {e}\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "339d7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   driverId  10 non-null     int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 212.0 bytes\n",
      "Scraping completado.\n"
     ]
    }
   ],
   "source": [
    "#3 Guardar datos\n",
    "\n",
    "df_scraped = pd.DataFrame(scraped_data)\n",
    "\n",
    "df_scraped.info()\n",
    "df_scraped.to_csv(\"../data/scraped/prueba02scraping.csv\", index = False)\n",
    "print(\"Scraping completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
